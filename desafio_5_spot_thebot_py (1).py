# -*- coding: utf-8 -*-
"""Desafio_5_SPOT_theBOT.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w-3ElKwn1AHcg8-Sra-EDEuvcDFn6o9X
"""

# Abrir VDB para RAC queries
import chromadb
chroma_client = chromadb.PersistentClient(path="/content/drive/MyDrive/i2a2/Desafio_5/Intellitools_vdb")
collection = chroma_client.get_collection(name="intellitools_collection")

# Abrir modelo

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
#from google.colab import output
#output.enable_custom_widget_manager()
from peft import PeftModel

#def LoadModel(base_model_id, bnb_config, qLoRA_checkpoint ):
#  model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config )
#  model = PeftModel.from_pretrained(model, qLoRA_checkpoint)
#  return model

#base_model_id = "mistralai/Mistral-7B-v0.1"

modelID = "nikinuk/spot_m7b_q4_qlora_cp50"

#qLoRA_checkpoint = "/content/drive/MyDrive/i2a2/Desafio_5/mistral-Intellitools/checkpoint-225"
#bnb_config = BitsAndBytesConfig(
#    load_in_4bit=True,
#    bnb_4bit_use_double_quant=True,
#    bnb_4bit_quant_type="nf4",
#    bnb_4bit_compute_dtype=torch.bfloat16)

#model = LoadModel(base_model_id, bnb_config, qLoRA_checkpoint)
model = LoadModel(modelID)

# Abrir tokenizer

#tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(modelID)
tokenizer.pad_token = tokenizer.eos_token

# Prompt Template

def prompter_RAC(prompt, name="Nicholas"):

    q_result = collection.query(
        query_texts=[prompt],
        n_results = 1,
        include =["distances", "documents", "metadatas"]
        )
    #print(q_result['distances'][0][0])
    if q_result['distances'][0][0] < 1:
        context = q_result['documents'][0][0]
        Template_Context_OK = f"""
System: VocÃª Ã© um chatbot muito prestativo chamado Spot. VocÃª responde Ã s perguntas do usuÃ¡rio, sempre em portuguÃªs, da melhor forma possÃ­vel usando o contexto apresentado.
Suas respostas devem ser curtas e diretas, usando as informaÃ§Ãµes do contexto.
NÃ£o havendo informaÃ§Ãµes suficiente no contexto, vocÃª deve direcionar o usuÃ¡rio ao atendimento personalizado.
VocÃª deve ser sempre amigÃ¡vel e profissional, seu objetivo Ã© ajudar o usuario a resolver um problema.

Contexto: {context}

Nome do UsuÃ¡rio: {name}

Prompt: {prompt}

Resposta:"""
        return Template_Context_OK
    else:
        Template_Context_NOK = f"""
System: VocÃª Ã© um chatbot muito prestativo chamado Spot. VocÃª responde Ã s perguntas do usuÃ¡rio, sempre em portuguÃªs, da melhor forma possÃ­vel usando o contexto apresentado.
Suas respostas devem ser curtas e diretas, ajudando o usuÃ¡rio a formular uma pergunta clara e relevante.
VocÃª deve ser sempre amigÃ¡vel e profissional, seu objetivo Ã© ajudar o usuario a explicar seu problema ou dÃºvida com poucas palavras.

Nome do UsuÃ¡rio: {name}

Prompt: {prompt}

Resposta:"""
        return Template_Context_NOK

# ASK ME ANYTHING

def AKA(prompt):
  model_input = tokenizer(prompter_RAC(prompt), return_tensors="pt").to("cuda")
  model.eval()
  with torch.no_grad():
    response = tokenizer.decode(model.generate(**model_input, max_length=2048, pad_token_id=2, repetition_penalty=1.3)[-1], skip_special_tokens=True)
  return response

# Streamlit flow

import streamlit as st
from PIL import Image

def update_assistant(old, new):
    # Adds new "assistent" entries in the chat context, limited by choosen ASSISTENT_MEMORY
    if len(old) > ASSISTENT_MEMORY:
        old.pop(0)
    old = old + [ { "role": "assistant", "content": new } ]
    return old

def set_avatar(user):
    if user == "user":
        return "ðŸ˜Ÿ"
    else:
        return p[psycho]["avatar"]

# FIXED SITE HEADER

# Initialize chat history
if "messages" not in st.session_state:
        st.session_state.messages = []
        st.session_state.gpt_assistant = []

        chat_response = "Bom dia, como posso ajudar?"

        # Display assistant response in chat message container
        with st.chat_message("Assistant"):
            st.markdown(chat_response)
        # Add assistant response to chat history
        st.session_state.messages.append({"role": "assistant", "content": chat_response})
        st.session_state.gpt_assistant = update_assistant(st.session_state.gpt_assistant, chat_response)

    # populate chat screen after first iteration
else:
        # Display chat messages from history on app rerun
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

   # React to user input
if prompt := st.chat_input("Me diga como posso ajudar."):
        # Display user message in chat message container
        with st.chat_message("User"):
            st.write(prompt)
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})
        # RUN GPT
        completion = AKA(prompter_RAC(prompt))
        chat_response = completion

        # Display assistant response in chat message container
        with st.chat_message("Assistant"):
            st.markdown(chat_response)
        # Add assistant response to chat history
        st.session_state.messages.append({"role": "assistant", "content": chat_response})
        st.session_state.gpt_assistant = update_assistant(st.session_state.gpt_assistant, chat_response)
