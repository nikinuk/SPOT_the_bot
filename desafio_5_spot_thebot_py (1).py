# -*- coding: utf-8 -*-
"""Desafio_5_SPOT_theBOT.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w-3ElKwn1AHcg8-Sra-EDEuvcDFn6o9X
"""

# Abrir VDB para RAC queries
import chromadb
chroma_client = chromadb.PersistentClient(path="/vdb_intellitools_wiki")
collection = chroma_client.get_collection(name="intellitools_collection")

# Abrir modelo

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel

def LoadModel(base_model_id, bnb_config, qLoRA_checkpoint ):
  model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config )
  model = PeftModel.from_pretrained(model, qLoRA_checkpoint)
  return model

base_model_id = "mistralai/Mistral-7B-v0.1"
qLoRA_checkpoint = "nikinuk/spot_m7b_q4_qlora_cp50"
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16)

model = LoadModel(base_model_id, bnb_config, qLoRA_checkpoint)

# Abrir tokenizer
tokenizer = AutoTokenizer.from_pretrained(qLoRA_checkpoint)
tokenizer.pad_token = tokenizer.eos_token

# Prompt Template

def prompter_RAC(prompt, name="Nicholas"):
    q_result = collection.query(
        query_texts=[prompt],
        n_results = 1,
        include =["distances", "documents", "metadatas"]
        )
    #print(q_result['distances'][0][0])
    if q_result['distances'][0][0] < 1:
        context = q_result['documents'][0][0]
        Template_Context_OK = f"""
System: Você é um chatbot muito prestativo chamado Spot. Você responde às perguntas do usuário, sempre em português, da melhor forma possível usando o contexto apresentado.
Suas respostas devem ser curtas e diretas, usando as informações do contexto.
Não havendo informações suficiente no contexto, você deve direcionar o usuário ao atendimento personalizado.
Você deve ser sempre amigável e profissional, seu objetivo é ajudar o usuario a resolver um problema.

Contexto: {context}

Nome do Usuário: {name}

Prompt: {prompt}

Resposta:"""
        return Template_Context_OK
    else:
        Template_Context_NOK = f"""
System: Você é um chatbot muito prestativo chamado Spot. Você responde às perguntas do usuário, sempre em português, da melhor forma possível usando o contexto apresentado.
Suas respostas devem ser curtas e diretas, ajudando o usuário a formular uma pergunta clara e relevante.
Você deve ser sempre amigável e profissional, seu objetivo é ajudar o usuario a explicar seu problema ou dúvida com poucas palavras.

Nome do Usuário: {name}

Prompt: {prompt}

Resposta:"""
        return Template_Context_NOK

# ASK ME ANYTHING

def AKA(prompt):
  model_input = tokenizer(prompter_RAC(prompt), return_tensors="pt").to("cuda")
  model.eval()
  with torch.no_grad():
    response = tokenizer.decode(model.generate(**model_input, max_length=2048, pad_token_id=2, repetition_penalty=1.3)[-1], skip_special_tokens=True)
  return response

# Streamlit flow

import streamlit as st
from PIL import Image

def update_assistant(old, new):
    # Adds new "assistent" entries in the chat context, limited by choosen ASSISTENT_MEMORY
    if len(old) > ASSISTENT_MEMORY:
        old.pop(0)
    old = old + [ { "role": "assistant", "content": new } ]
    return old

# FIXED SITE HEADER

# Initialize chat history
if "messages" not in st.session_state:
        st.session_state.messages = []
        st.session_state.gpt_assistant = []

        chat_response = "Bom dia, como posso ajudar?"

        # Display assistant response in chat message container
        with st.chat_message("Assistant"):
            st.markdown(chat_response)
        # Add assistant response to chat history
        st.session_state.messages.append({"role": "assistant", "content": chat_response})
        st.session_state.gpt_assistant = update_assistant(st.session_state.gpt_assistant, chat_response)

    # populate chat screen after first iteration
else:
        # Display chat messages from history on app rerun
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

   # React to user input
if prompt := st.chat_input("Me diga como posso ajudar."):
        # Display user message in chat message container
        with st.chat_message("User"):
            st.write(prompt)
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})
        # RUN MODEL
        completion = AKA(prompter_RAC(prompt))
        chat_response = completion

        # Display assistant response in chat message container
        with st.chat_message("Assistant"):
            st.markdown(chat_response)
        # Add assistant response to chat history
        st.session_state.messages.append({"role": "assistant", "content": chat_response})
        st.session_state.gpt_assistant = update_assistant(st.session_state.gpt_assistant, chat_response)
